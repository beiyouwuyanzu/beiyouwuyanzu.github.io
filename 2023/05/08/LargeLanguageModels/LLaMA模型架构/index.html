<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		LLaMA的模型架构——RMSNorm/SwiGLU/RoPE/Transformer | 
	 
	Wangyaqi&#39;s personal site [notice:图片浏览须科学上网,公式渲染需要刷新]
	</title>
	
	<!-- keywords,description -->
	 
		<meta name="description" content="about study notes" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	


	<!-- search -->
	<script>
		var searchEngine = "https://www.google.com/search?q=";
		if(typeof searchEngine == "undefined" || searchEngine == null || searchEngine == ""){
			searchEngine = "https://www.google.com/search?q=";
		}
		var homeHost = "wujun234.github.io";
		if(typeof homeHost == "undefined" || homeHost == null || homeHost == ""){
			homeHost = window.location.host;
		}
	</script>


	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">


	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@v1.4.14/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	

	<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3efe99c287df5a1d6f0d02d187e403c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<header id="header">
    <a id="title" href="/" class="logo">Wangyaqi's personal site [notice:图片浏览须科学上网,公式渲染需要刷新]</a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
	
		<li class="menu-item">
			<a href="/tags" class="menu-item-link">标签</a>
		</li>
	

	
		<li class="menu-item">
			<a href="/categories" class="menu-item-link">分类</a>
		</li>
	

		<li class="menu-item">
			<a href="https://github.com/wujun234/uid-generator-spring-boot-starter" class="menu-item-link" target="_blank">
				UidGenerator
			</a>
		</li>
		<li class="menu-item">
			<a href="https://github.com/wujun234" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="search" placeholder="按回车全站搜索">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										LargeLanguageModels
									</a>
									
							<ul>
								<li class="file active">
									<a href="/2023/05/08/LargeLanguageModels/LLaMA%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/">
                     
										    LLaMA模型架构
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/LoRA/">
                     
										    LoRA
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/alpaca_LoRA%E5%AE%9E%E7%8E%B0/">
                     
										    alpaca_LoRA实现
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/LargeLanguageModels/llama_bloom_chatglm%E5%8C%BA%E5%88%AB/">
                     
										    llama_bloom_chatglm区别
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/llama_visualization/">
                     
										    llama_visualization
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/peft_model/">
                     
										    peft_model
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/transformer/">
                     
										    transformer
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/LargeLanguageModels/xlnet/">
                     
										    xlnet
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										机器学习
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										kaggle_note
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kaggle_note/text_classification/">
                     
										    text_classification
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										nlp
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp/crf/">
                     
										    crf
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										paper
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/paper/asr%E8%AF%84%E4%BC%B0/">
                     
										    asr评估
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										深度学习
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GBRANK/">
                     
										    GBRANK
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/MMoE/">
                     
										    MMoE
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attention/">
                     
										    attention
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/q-learning/">
                     
										    q-learning
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BC%98%E5%8C%96%E5%99%A8/">
                     
										    优化器
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
                     
										    常见网络的代码实现
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0ppo/">
                     
										    强化学习ppo
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">
                     
										    损失函数
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
                     
										    激活函数
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">
                     
										    知识蒸馏
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										项目集合
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/llama_%E5%B0%8F%E5%AD%A6%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%A2%98%E6%A8%A1%E5%9E%8B/">
                     
										    llama_小学数学解题模型
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/stableDiffusion/">
                     
										    stableDiffusion
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">
	LLaMA的模型架构——RMSNorm/SwiGLU/RoPE/Transformer
</h1>
<div class="article-meta">
	
		<span>
			阅读量:<span id="/2023/05/08/LargeLanguageModels/LLaMA%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/" class="leancloud_visitors" data-flag-title="LLaMA的模型架构——RMSNorm/SwiGLU/RoPE/Transformer"></span>
		</span>
	
	<span>wang yaqi</span>
	<span>2023-05-08 21:04:50</span>
		<div id="article-categories">
    
		<span>Categories：</span>
            
    

    
		<span>Tags：</span>
            
    
		</div>

</div>

<div id="article-content">
	<h2 id="rmsnorm均方根root-mean-square对每个transformer子层的输入进行归一化">1. RMSNorm(均方根)（Root Mean Square）：对每个Transformer子层的输入进行归一化</h2>
<p>RMSNorm计算公式: <span class="math inline">\(y = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2+\epsilon}}\)</span></p>
<p>其中，<span class="math inline">\(x\)</span>是输入向量，<span class="math inline">\(y\)</span>是输出向量，<span class="math inline">\(n\)</span>是向量的维度，<span class="math inline">\(ϵ\)</span>是一个很小的常数，用于避免分母为0的情况。 #### 一般的LN: <span class="math display">\[\bar{a}_i=\frac{a_i-\mu}{\sigma} g_i\]</span> 其中 <span class="math display">\[\mu=\frac{1}{n} \sum_{i=1}^n a_i\]</span></p>
<p><span class="math display">\[\sigma=\sqrt{\frac{1}{n} \sum_{i=1}^n\left(a_i-\mu\right)^2}\]</span></p>
<h4 id="rms-norm">RMS Norm：</h4>
<p><span class="math display">\[\bar{a}_i=\frac{a_i}{R M S(a)} g_i\]</span></p>
<p>其中</p>
<p><span class="math display">\[R M S(a)=\sqrt{\frac{1}{n} \sum_{i=1}^n a_i^2}\]</span></p>
<h3 id="特点">特点</h3>
<ol style="list-style-type: decimal">
<li>RMS Norm是一般LayerNorm的一种变体，可以在梯度下降时令损失更加平滑<br />
与layerNorm相比，RMS Norm的主要区别在于去掉了减去均值的部分(re-centering)，只保留方差部分(re-scaling)</li>
<li>RMS Norm是一种神经网络的归一化方法，可以提高模型的泛化性能。具体而言，RMS Norm是对每个神经元的输出做归一化，使得每个神经元的输出的均值为0，方差为1。这种归一化方法可以使得每个神经元的输出分布更加稳定，降低了不同神经元之间的依赖关系，提高了模型的鲁棒性和泛化性能。</li>
<li>RMS Norm的优点在于，它可以有效地降低神经元之间的依赖关系，提高了模型的鲁棒性和泛化性能。另外，RMS Norm与其他归一化方法不同，它不需要对每个mini-batch进行统计，因此可以在训练集较小的情况下获得更好的效果。</li>
</ol>
<h2 id="swiglu">2. SwiGLU</h2>
<blockquote>
<p>激活函数</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><p>relu: 𝑅𝑒𝐿𝑈(𝑥)=𝑚𝑎𝑥(0,𝑥)</p></li>
<li><p>GeLU: <span class="math inline">\(G e L U=x \Phi(x)=x \int_{-\infty}^x \frac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} d t=x \cdot \frac{1}{2}\left[1+\operatorname{erf}\left(\frac{x}{\sqrt{2}}\right)\right]\)</span></p>
其中erf为误差函数</li>
<li>Swish激活函数： Swish <span class="math inline">\(=x \cdot \operatorname{sigmoid}(\beta x)\)</span> 激活函数就是对x乘以一些数，以对某些值进行约束</li>
<li>GLU（Gated Linear Unit），其一般形式为： <span class="math inline">\(G L U(x)=\sigma(W x+b) \otimes(V x+c)\)</span></li>
</ol>
<ul>
<li>这里的𝜎可以是𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数，也可以是其它的一些激活函数，其相关变体如下： <span class="math inline">\(\begin{aligned} &amp; G T U(x, W, V, b, c)=\tanh (x W+b) \otimes \sigma(x V+c) \\ &amp; \operatorname{Bilinear}(x, W, V, b, c)=(x W+b) \otimes(x V+c) \\ &amp; \operatorname{Re} G L U(x, W, V, b, c)=\operatorname{Re} L U(x W+b) \otimes(x V+c) \\ &amp; G E G L U(x, W, V, b, c)=G E L U(x W+b) \otimes(x V+c)\end{aligned}\)</span> <span class="math inline">\(\operatorname{SwiGLU}(x, W, V, b, c, \beta)=\operatorname{Swish}_\beta(x W+b) \otimes(x V+c)\)</span></li>
</ul>
<div class="figure">
<img src="https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305081930944.png" />

</div>
<pre><code>                  左：GeLU，右：Swish</code></pre>
<hr />
<blockquote>
<p>https://www.ai-contentlab.com/2023/03/swishglu-activation-function.html</p>
</blockquote>
<ul>
<li>Swish(x) = x * sigmoid(beta * x)</li>
</ul>
<p><code>Swish 已被证明在许多应用程序中表现优于 ReLU，尤其是在深度网络中。 Swish 的主要优点是它比 ReLU 更平滑，可以带来更好的优化和更快的收敛。</code> - GLU(x) = x * sigmoid(Wx + b) <code>GLU 与 Swish 类似，它结合了线性函数和非线性函数。然而，在 GLU 中，线性函数由 sigmoid 激活函数门控。</code></p>
<ul>
<li>SwiGLU(x) = x * sigmoid(beta * x) + (1 - sigmoid(beta * x)) * (Wx + b) <code>在SwiGLU中，Swish函数用于对GLU的线性函数进行门控。这使得 SwiGLU 可以兼顾 Swish 和 GLU 的优势，同时克服各自的劣势。</code></li>
</ul>
<h3 id="特点-1">特点</h3>
<ol style="list-style-type: decimal">
<li>Swish 的主要优点是它比 ReLU 更平滑，可以带来更好的优化和更快的收敛</li>
<li>GLU 与 Swish 类似，它结合了线性函数和非线性函数。然而，在 GLU 中，线性函数由 sigmoid 激活函数门控。</li>
<li>在SwiGLU中，Swish函数用于对GLU的线性函数进行门控。这使得 SwiGLU 可以兼顾 Swish 和 GLU 的优势，同时克服各自的劣势。</li>
</ol>
<h3 id="与其他激活函数相比swiglu-的主要优点是">与其他激活函数相比，SwiGLU 的主要优点是：</h3>
<ol style="list-style-type: decimal">
<li>平滑度：SwiGLU 比 ReLU 更平滑，可以带来更好的优化和更快的收敛。</li>
<li>非单调性：SwiGLU 是非单调的，这使其能够捕获输入和输出之间复杂的非线性关系。</li>
<li>门控：SwiGLU 使用门控机制，允许它根据接收到的输入有选择地激活神经元。这有助于减少过度拟合并提高泛化能力。</li>
<li>性能：SwiGLU 已被证明在各种任务中优于其他激活函数，包括 Swish 和 GLU。</li>
</ol>
<h3 id="代码实现">代码实现</h3>
<pre><code>class SwiGLU(tf.keras.layers.Layer):
    def __init__(self, bias=True, dim=-1, **kwargs):
        &quot;&quot;&quot;
 SwiGLU Activation Layer
 &quot;&quot;&quot;
        super(SwiGLU, self).__init__(**kwargs)
        self.bias = bias
        self.dim = dim
        self.dense = tf.keras.layers.Dense(2, use_bias=bias)

    def call(self, x):
        out, gate = tf.split(x, num_split=2, axis=self.dim)
        gate = tf.keras.activations.swish(gate)
        x = tf.multiply(out, gate)
        return x</code></pre>
<h2 id="rope">3. RoPE</h2>
<ol style="list-style-type: decimal">
<li>RoPE 使用旋转矩阵对绝对位置进行编码，同时将显式相对位置依赖性纳入自注意公式中。</li>
<li>RoPE 具有多个优点：序列长度的灵活性、随着相对距离的增加而衰减的令牌间依赖性，以及为线性自注意力配备相对位置编码的能力。</li>
</ol>
<p>在位置编码上，删除了绝对位置嵌入，而在网络的每一层增加了苏剑林等人(2021)提出的旋转位置嵌入(RoPE)，其思想是采用绝对位置编码的形式，实现相对位置编码</p>
<p>RoPE主要借助了复数的思想，为了引入复数，首先假设了在加入位置信息之前，原有的编码向量是二维行向量<span class="math inline">\(q_m\)</span>和<span class="math inline">\(k_n\)</span>，其中<span class="math inline">\(m\)</span>和<span class="math inline">\(n\)</span>是绝对位置，现在需要构造一个变换，将<span class="math inline">\(m\)</span>和<span class="math inline">\(n\)</span>引入到<span class="math inline">\(q_m\)</span>和<span class="math inline">\(k_n\)</span>中，即寻找变换： <span class="math display">\[
\tilde{q_m}=f(q, m), \tilde{k_n}=f(k, n)
\]</span></p>
<p>考虑到Attention的核心计算是内积： <span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
\]</span></p>
<p>所以，寻求的这个<span class="math inline">\(f(*)\)</span>变换，应该具有特性： <span class="math display">\[
\langle f(q, m), f(k, n)\rangle=g(q, k, m-n)
\]</span></p>
<p>这里直接说结论，寻求的变换就是<span class="math inline">\(q_me^{im\theta}\)</span> 也就是给<span class="math inline">\(q_m\)</span>乘以<span class="math inline">\(e^{im\theta}\)</span>，相应地，<span class="math inline">\(k_n\)</span>乘以<span class="math inline">\(e^{in\theta}\)</span></p>
<p>做了这样一个变换之后，根据复数的特性，有： <span class="math display">\[
\left\langle q_m, k_n\right\rangle=\operatorname{Re}\left[q_m k_n^*\right]
\]</span></p>
<p>也就是，如果把二维向量看做复数，那么它们的内积，等于一个复数乘以另一个复数的共轭，得到的结果再取实部，代入上面的变换，也就有： <span class="math display">\[
\left\langle q_m e^{i m \theta}, k_n e^{i n \theta}\right\rangle=\operatorname{Re}\left[\left(q_m e^{i m \theta}\right)\left(k_n e^{i n \theta}\right)^*\right]=\operatorname{Re}\left[q_m k_n^* e^{i(m-n) \theta}\right]
\]</span></p>
<p>这样一来，内积的结果就只依赖于<span class="math inline">\((m-n)\)</span>，也就是相对位置了</p>
<p>换言之，经过这样一番操作，通过给Embedding添加绝对位置信息，可以使得两个token的编码，经过内积变换（self-attn）之后，得到结果是受它们位置的差值，即相对位置影响的</p>
<p>于是对于任意的位置为<span class="math inline">\(m\)</span>的二维向量<span class="math inline">\([x, y]\)</span>，把它看做复数，乘以<span class="math inline">\(e^{im\theta}\)</span>，而根据欧拉公式，有： <span class="math display">\[
e^{i m \theta}=\cos m \theta+i \sin m \theta
\]</span></p>
<p>于是上述的相乘变换也就变成了： <span class="math display">\[
(x+i y) e^{i m \theta}=(x \cos m \theta-y \sin m \theta)+i(x \sin m \theta+y \cos m \theta)
\]</span></p>
<p>把上述式子写成矩阵形式： <span class="math display">\[
f\left(\left(q_0, q_1\right), m\right)=\left[\begin{array}{cc}
\cos m \theta &amp; -\sin m \theta \\
\sin m \theta &amp; \cos m \theta
\end{array}\right]\left[\begin{array}{l}
q_0 \\
q_1
\end{array}\right]
\]</span></p>
<p>而这个变换的几何意义，就是在二维坐标系下，对向量<span class="math inline">\((q_0, q_1)\)</span>进行了旋转，因而这种位置编码方法，被称为旋转位置编码</p>
<p>根据刚才的结论，结合内积的线性叠加性，可以将结论推广到高维的情形。可以理解为，每两个维度一组，进行了上述的“旋转”操作，然后再拼接在一起： <span class="math display">\[
\left[\begin{array}{ccccccc}
\cos m \theta_0 &amp; -\sin m \theta_0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\sin m \theta_0 &amp; \cos m \theta_0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos m \theta_1 &amp; -\sin m \theta_1 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin m \theta_1 &amp; \cos m \theta_1 &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m \theta_{d / 2-1} &amp; -\sin m \theta_{d / 2-1} \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m \theta_{d / 2-1} &amp; \cos m \theta_{d / 2-1}
\end{array}\right]\left[\begin{array}{c}
q_0 \\
q_1 \\
q_2 \\
q_3 \\
\vdots \\
q_{d-2} \\
q_{d-1}
\end{array}\right]
\]</span></p>
<p>由于矩阵的稀疏性，会造成计算上的浪费，所以在计算时采用逐位相乘再相加的方式进行：</p>
<p><span class="math display">\[
\left[\begin{array}{c}
q_0 \\
q_1 \\
q_2 \\
q_3 \\
\vdots \\
q_{d-2} \\
q_{d-1}
\end{array}\right] \otimes\left[\begin{array}{c}
\cos m \theta_0 \\
\cos m \theta_0 \\
\cos m \theta_1 \\
\cos m \theta_1 \\
\vdots \\
\cos m \theta_{d / 2-1} \\
\cos m \theta_{d / 2-1}
\end{array}\right]+\left[\begin{array}{c}
-q_1 \\
q_0 \\
-q_3 \\
q_2 \\
\vdots \\
-q_{d-1} \\
q_{d-2}
\end{array}\right] \otimes\left[\begin{array}{c}
\sin m \theta_0 \\
\sin m \theta_0 \\
\sin m \theta_1 \\
\sin m \theta_1 \\
\vdots \\
\sin m \theta_{d / 2-1} \\
\sin m \theta_{d / 2-1}
\end{array}\right]
\]</span></p>
<hr />
<p>另一份解释: &gt; https://nn.labml.ai/transformers/rope/index.html</p>
<p>旋转编码通过在 2D 平面中旋转来变换特征对。也就是说，它将 d 特征组织为 d/2​ 对。每对都可 以被认为是 2D 平面中的一个坐标，编码将根据令牌的位置将其旋转一个角度。 设 <span class="math inline">\(x_m^{(1)}\)</span>和 <span class="math inline">\(x_m^{(2)}\)</span> 是 m 位置处任意头的键或查询的两个特征。或者为简单起见，假设 x 只有两个特征。那么转变就是， <span class="math display">\[
\begin{aligned}
\operatorname{RoPE}\left(x_m^{(1)}, x_m^{(2)}, m\right) &amp; =\left(\begin{array}{cc}
\cos m \theta &amp; -\sin m \theta \\
\sin m \theta &amp; \cos m \theta
\end{array}\right)\left(\begin{array}{l}
x_m^{(1)} \\
x_m^{(2)}
\end{array}\right) \\
&amp; =\left(\begin{array}{c}
x_m^{(1)} \cos m \theta-x_m^{(2)} \sin m \theta \\
x_m^{(2)} \cos m \theta+x_m^{(1)} \sin m \theta
\end{array}\right)
\end{aligned}
\]</span></p>
<p>这些特征被分组成对并按上述方式处理。他们对每对使用不同的 θ 。 该论文建议对 d/2​ 特征对使用 <span class="math display">\[
\Theta=\theta_i=10000^{\frac{2(i-1)}{d}}, i \in\left[1,2, \ldots, \frac{d}{2}\right]
\]</span></p>
<p>我们将特征 i 与特征 i+d/2​ 配对。所以对于位置 m ，我们转换 <span class="math display">\[
\left(\begin{array}{c}
x_m^{(i)} \\
x_m^{\left(i+\frac{d}{2}\right)}
\end{array}\right)
\]</span> to <span class="math display">\[
\left(\begin{array}{l}
x_m^{(i)} \cos m \theta_i-x_m^{\left(i+\frac{d}{2}\right)} \sin m \theta_i \\
x_m^{\left(i+\frac{d}{2}\right)} \cos m \theta_i+x_m^{(i)} \sin m \theta_i
\end{array}\right)
\]</span></p>
<h3 id="补充">补充</h3>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.eleuther.ai/rotary-embeddings/" class="uri">https://blog.eleuther.ai/rotary-embeddings/</a></p>
<p>简单来说，两个向量之间的点积是单个向量的大小和它们之间的角度的函数。考虑到这一点，RoPE 背后的直觉是我们可以将令牌嵌入表示为复数，并将它们的位置表示为我们应用于它们的纯旋转。如果我们将查询和键移动相同的量，改变绝对位置而不是相对位置，这将导致两个表示以相同的方式额外旋转——正如我们将在推导中看到的——因此角度它们之间将保持不变，因此点积也将保持不变。通过利用旋转的性质，self-attention 中使用的点积将具有我们正在寻找的属性，保留相对位置信息，同时丢弃绝对位置。</p>
</blockquote>
<div class="figure">
<img src="https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/3D217A91-3E8D-4E6A-96B6-2245A4A39426.png" />

</div>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  激活函数
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/2023/05/08/LargeLanguageModels/LoRA/">
                LoRA
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			el: '#vcomments',
			appId: 'IGK6A3UjpP5uO7JWtA2JoBuS-gzGzoHsz',
			appKey: 'RRf6JtF145uakqoa7Hv1Ahr8',
			placeholder: '请输入评论',
			path: window.location.pathname,
			avatar: 'retro',
			highlight: false,
      recordIP: true,
      enableQQ: true,
			requiredFields: ['nick','mail']
		})
	
	
    // 显示次数
		function showTime(Counter) {
			var query = new AV.Query("Counter");
			if($(".leancloud_visitors").length > 0){
				var url = $(".leancloud_visitors").attr('id').trim();
				// where field
				query.equalTo("words", url);
				// count
				query.count().then(function (number) {
					// There are number instances of MyClass where words equals url.
					$(document.getElementById(url)).text(number?  number : '--');
				}, function (error) {
					// error is an instance of AVError.
				});
			}
		}
		// 追加pv
		function addCount(Counter) {
			var url = $(".leancloud_visitors").length > 0 ? $(".leancloud_visitors").attr('id').trim() : 'wujun234.github.io';
			var Counter = AV.Object.extend("Counter");
			var query = new Counter;
			query.save({
				words: url
			}).then(function (object) {
			})
		}
		$(function () {
			var Counter = AV.Object.extend("Counter");
			addCount(Counter);
			showTime(Counter);
		});
	
</script>
	</div>
	<div id="footer">
	<p>
	©2019-<span id="footerYear"></span> 
	<a href="/">wang yaqi</a>
	|<a href="https://beian.miit.gov.cn" target="_blank">京ICP备2022000211号-1</a>	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>