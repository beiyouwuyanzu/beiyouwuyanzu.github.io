<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		损失函数 | 
	 
	Wangyaqi&#39;s personal site [notice:图片浏览须科学上网,公式渲染需要刷新]
	</title>
	
	<!-- keywords,description -->
	 
		<meta name="description" content="about study notes" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	


	<!-- search -->
	<script>
		var searchEngine = "https://www.google.com/search?q=";
		if(typeof searchEngine == "undefined" || searchEngine == null || searchEngine == ""){
			searchEngine = "https://www.google.com/search?q=";
		}
		var homeHost = "wujun234.github.io";
		if(typeof homeHost == "undefined" || homeHost == null || homeHost == ""){
			homeHost = window.location.host;
		}
	</script>


	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">


	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@v1.4.14/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	

	<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3efe99c287df5a1d6f0d02d187e403c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<header id="header">
    <a id="title" href="/" class="logo">Wangyaqi's personal site [notice:图片浏览须科学上网,公式渲染需要刷新]</a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
	
		<li class="menu-item">
			<a href="/tags" class="menu-item-link">标签</a>
		</li>
	

	
		<li class="menu-item">
			<a href="/categories" class="menu-item-link">分类</a>
		</li>
	

		<li class="menu-item">
			<a href="https://github.com/wujun234/uid-generator-spring-boot-starter" class="menu-item-link" target="_blank">
				UidGenerator
			</a>
		</li>
		<li class="menu-item">
			<a href="https://github.com/wujun234" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="search" placeholder="按回车全站搜索">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										LargeLanguageModels
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/LLaMA%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/">
                     
										    LLaMA模型架构
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/LoRA/">
                     
										    LoRA
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/alpaca_LoRA%E5%AE%9E%E7%8E%B0/">
                     
										    alpaca_LoRA实现
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/LargeLanguageModels/llama_bloom_chatglm%E5%8C%BA%E5%88%AB/">
                     
										    llama_bloom_chatglm区别
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/llama_visualization/">
                     
										    llama_visualization
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/peft_model/">
                     
										    peft_model
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/transformer/">
                     
										    transformer
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/LargeLanguageModels/xlnet/">
                     
										    xlnet
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										机器学习
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										kaggle_note
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kaggle_note/text_classification/">
                     
										    text_classification
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										nlp
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp/crf/">
                     
										    crf
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										paper
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/paper/asr%E8%AF%84%E4%BC%B0/">
                     
										    asr评估
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										深度学习
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GBRANK/">
                     
										    GBRANK
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/MMoE/">
                     
										    MMoE
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attention/">
                     
										    attention
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/q-learning/">
                     
										    q-learning
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BC%98%E5%8C%96%E5%99%A8/">
                     
										    优化器
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
                     
										    常见网络的代码实现
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0ppo/">
                     
										    强化学习ppo
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file active">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">
                     
										    损失函数
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
                     
										    激活函数
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">
                     
										    知识蒸馏
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										项目集合
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/llama_%E5%B0%8F%E5%AD%A6%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%A2%98%E6%A8%A1%E5%9E%8B/">
                     
										    llama_小学数学解题模型
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/stableDiffusion/">
                     
										    stableDiffusion
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">
	损失函数
</h1>
<div class="article-meta">
	
		<span>
			阅读量:<span id="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="leancloud_visitors" data-flag-title="损失函数"></span>
		</span>
	
	<span>wang yaqi</span>
	<span>2023-05-08 21:04:50</span>
		<div id="article-categories">
    
		<span>Categories：</span>
            
    

    
		<span>Tags：</span>
            
    
		</div>

</div>

<div id="article-content">
	<h2 id="均方误差">1. 均方误差</h2>
<p>均方误差(MSE，Mean Squared Error)：均方误差是回归任务中最常用的损失函数之一，用于衡量模型的输出与真实值之间的平均差的平方。 <span class="math display">\[L(Y|f(x)) = (1/n) * ∑(Yi - f(xi))^2\]</span></p>
<p>其中<span class="math inline">\(Yi\)</span>是真实值，<span class="math inline">\(f(xi)\)</span>是模型预测值，<span class="math inline">\(n\)</span>是样本数量。MSE的值越小，表示预测模型描述的样本数据具有越好的精确度</p>
<p>MSE损失函数的优点包括： - 无参数，计算成本低； - 具有明确的物理意义，是一种优秀的距离度量方法。</p>
<p>MSE损失函数的缺点包括： - 在某些应用场景下表现较弱，例如在图像和语音处理方面； - 对异常值（outliers）比较敏感，可能会导致模型过度拟合。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np

<span class="kw">def</span> mean_squared_error(y_true, y_pred):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    均方误差（MSE）损失函数的实现</span>
<span class="co">    &quot;&quot;&quot;</span>
    mse <span class="op">=</span> np.mean(np.power(y_true <span class="op">-</span> y_pred, <span class="dv">2</span>))
    <span class="cf">return</span> mse</code></pre></div>
<h2 id="交叉熵损失函数">2. 交叉熵损失函数</h2>
<p>交叉熵损失函数是一种用于在机器学习中衡量预测值与实际值之间差距的损失函数。交叉熵通常用于分类问题中，其中我们想要将输入数据分成多个不同的类别。在分类问题中，我们希望模型的输出尽可能接近真实标签。我们可以使用交叉熵损失函数来计算模型预测的概率分布与真实标签的差距，从而衡量模型的准确性。</p>
<p>在二元分类问题中，交叉熵损失函数可以写成以下形式： <span class="math display">\[ L = -\frac{1}{N}\sum_{i=1}^{N} (t_i\log(p_i) + (1-t_i)\log(1-p_i)) \]</span></p>
<p>其中，<span class="math inline">\(t_i\)</span> 是真实标签，<span class="math inline">\(p_i\)</span> 是模型预测的概率，<span class="math inline">\(N\)</span> 是样本数量。当真实标签 <span class="math inline">\(t_i\)</span> 为 0 时，随着预测概率 <span class="math inline">\(p_i\)</span> 趋近于 0，交叉熵损失趋近于 0。</p>
<p>在多类分类问题中，交叉熵损失函数可以写成以下形式： <span class="math display">\[ L = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{K} y_{i,j}\log(\hat{y}_{i,j}) \]</span></p>
<p>其中，<span class="math inline">\(K\)</span> 表示类别的数量，<span class="math inline">\(y_{i,j}\)</span> 表示第 <span class="math inline">\(i\)</span> 个样本的第 <span class="math inline">\(j\)</span> 个类别的真实标签，<span class="math inline">\(\hat{y}_{i,j}\)</span> 表示模型对于第 <span class="math inline">\(i\)</span> 个样本的第 <span class="math inline">\(j\)</span> 个类别的预测概率值。交叉熵损失函数的目标仍然是最小化预测与实际标签之间的差距，从而让模型能够更准确地进行分类。</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np

<span class="kw">def</span> binary_cross_entropy_loss(y_true, y_pred):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    计算二元交叉熵损失函数</span>
<span class="co">    :param y_true: 真实标签，维度为 (m, 1)</span>
<span class="co">    :param y_pred: 预测概率，维度为 (m, 1)</span>
<span class="co">    :return: 二元交叉熵损失函数值</span>
<span class="co">    &quot;&quot;&quot;</span>
    epsilon <span class="op">=</span> <span class="fl">1e-7</span>  <span class="co"># 避免出现除以 0 的情况</span>
    y_pred <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span>. <span class="op">-</span> epsilon)  <span class="co"># 对预测概率进行修剪</span>
    loss <span class="op">=</span> <span class="op">-</span> (y_true <span class="op">*</span> np.log(y_pred) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> y_true) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> y_pred))  <span class="co"># 计算交叉熵损失</span>
    <span class="cf">return</span> np.mean(loss)  <span class="co"># 返回平均损失值</span>




<span class="kw">def</span> categorical_cross_entropy_loss(y_true, y_pred):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    计算多类交叉熵损失函数</span>
<span class="co">    :param y_true: 真实标签，维度为 (m, k)</span>
<span class="co">    :param y_pred: 预测概率，维度为 (m, k)</span>
<span class="co">    :return: 多类交叉熵损失函数值</span>
<span class="co">    &quot;&quot;&quot;</span>
    epsilon <span class="op">=</span> <span class="fl">1e-7</span>  <span class="co"># 避免出现除以 0 的情况</span>
    y_pred <span class="op">=</span> np.clip(y_pred, epsilon, <span class="dv">1</span>. <span class="op">-</span> epsilon)  <span class="co"># 对预测概率进行修剪</span>
    loss <span class="op">=</span> <span class="op">-</span> np.mean(np.<span class="bu">sum</span>(y_true <span class="op">*</span> np.log(y_pred), axis<span class="op">=</span><span class="dv">1</span>))  <span class="co"># 计算交叉熵损失</span>
    <span class="cf">return</span> loss</code></pre></div>
<h2 id="感知机损失函数">3. 感知机损失函数</h2>
<p>感知机损失函数是一种二分类的损失函数，它的定义如下：<span class="math display">\[L(y,f(x)) = max(0,-yf(x))\]</span> 其中，<span class="math inline">\(y\)</span> 表示样本的真实标签，<span class="math inline">\(f(x)\)</span> 表示样本的预测值。当 <span class="math inline">\(yf(x)&gt;0\)</span> 时，说明预测正确，损失为 <span class="math inline">\(0\)</span>；当 <span class="math inline">\(yf(x)&lt;0\)</span> 时，说明预测错误，损失为 <span class="math inline">\(-yf(x)\)</span>。感知机损失函数是一个非凸函数，因此不能使用梯度下降法求解最优解。但是可以使用随机梯度下降法求解最优解[^1^][3]。</p>
<p>以下是一个用 Python 实现感知机损失函数的例子：</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> perceptron_loss(y_true, y_pred):
    <span class="cf">return</span> K.maximum(<span class="dv">0</span>., <span class="op">-</span>y_true <span class="op">*</span> y_pred)</code></pre></div>
<p>其中，<code>K.maximum()</code> 函数返回两个张量中元素级别的最大值。如果两个张量中的元素不同，则返回一个张量，其中每个元素都是两个张量中相应元素的最大值。如果两个张量中的元素相同，则返回一个张量，其中每个元素都等于这些元素中的任意一个.</p>
<h2 id="kl散度损失">4. KL散度损失</h2>
<p>KL散度是一种用于度量概率分布之间差异的方法，常用于监督学习和强化学习中的损失函数中，也被称为相对熵。KL散度的计算公式如下：<span class="math inline">\(D_{KL}(P||Q)=\sum_{i=1}^{n}p_i\log\frac{p_i}{q_i}\)</span>，其中 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 分别是两个概率分布，<span class="math inline">\(p_i\)</span> 和 <span class="math inline">\(q_i\)</span> 分别是 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 在第 <span class="math inline">\(i\)</span> 个事件上的概率。KL散度的值越小，表示 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 越相似。KL散度的值为 <span class="math inline">\(0\)</span> 时，表示 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 完全相同。</p>
<p>在监督学习中，KL散度可以用作损失函数，用于度量模型输出的概率分布与真实标签的概率分布之间的差异。KL散度作为损失函数的优点是可以避免类别不平衡问题，缺点是可能存在梯度消失的问题。</p>
<p>下面是使用Python实现KL散度损失函数的代码示例：</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> tensorflow <span class="im">as</span> tf

<span class="kw">def</span> kl_divergence_loss(y_true, y_pred):
    kl_loss <span class="op">=</span> tf.keras.losses.KLDivergence()
    <span class="cf">return</span> kl_loss(y_true, y_pred)</code></pre></div>
<p>上述代码使用了TensorFlow中的KLDivergence()函数来计算KL散度损失。其中，y_true表示真实标签的概率分布，y_pred表示模型输出的概率分布。</p>
<h2 id="hinge损失和感知机损失区别">5. hinge损失和感知机损失区别</h2>
<p>Hinge损失函数和感知机损失函数有一定联系，因为它们都是支持向量机（SVM）算法中使用的损失函数。感知机算法可以看做是最简单的线性分类模型，而SVM则是在感知机算法的基础上进一步发展和改进而来的。</p>
<p>感知机损失函数和Hinge损失函数都是用于二元分类任务的损失函数，目标是将正负样本正确分类，并最小化误分类样本的数量。感知机损失函数和Hinge损失函数的表达式如下：</p>
<p>感知机损失函数： <span class="math inline">\(L(y, \hat{y}) = \max(0, -y\hat{y})\)</span></p>
<p>Hinge损失函数： <span class="math inline">\(L(y, \hat{y}) = \max(0, 1-y\hat{y})\)</span></p>
<p>其中，<span class="math inline">\(y\)</span>表示样本的真实标签，<span class="math inline">\(\hat{y}\)</span>表示样本的预测标签。</p>
<p>感知机损失函数和Hinge损失函数的主要区别在于对误分类样本的处理方式。感知机损失函数对误分类样本的惩罚是线性的，而Hinge损失函数对误分类样本的惩罚是非线性的，它会惩罚离正确分类较远的误分类样本。这种非线性惩罚机制使得SVM算法更加鲁棒，可以处理更加复杂的分类问题。</p>
<p>可以看出，Hinge损失函数是感知机损失函数的一种改进形式，是SVM算法的核心部分之一。在SVM算法中，通过最小化Hinge损失函数来得到最优的分类超平面。</p>
<h2 id="余弦相似度损失">6. 余弦相似度损失</h2>
<p>余弦相似度是一种用于计算两个向量之间的相似度的度量方法。在机器学习中，常常使用余弦相似度来评估两个向量之间的相似度，比如在聚类、分类、推荐系统等领域中。</p>
<p>余弦相似度损失函数的原理是，对于两个向量<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>，我们可以计算它们的余弦相似度<span class="math inline">\(cossim(u,v)\)</span>，然后将它们的余弦相似度作为模型的损失函数，我们可以使用以下公式计算两个向量之间的余弦相似度：</p>
<p><span class="math inline">\(cossim(u,v) = \frac{u \cdot v}{||u|| \cdot ||v||}\)</span></p>
<p>其中，<span class="math inline">\(u \cdot v\)</span>表示向量<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>的内积，<span class="math inline">\(||u||\)</span>和<span class="math inline">\(||v||\)</span>分别表示向量<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>的模长。余弦相似度的取值范围为<span class="math inline">\([-1,1]\)</span>，当两个向量相似度越高时，余弦相似度越接近1；反之，当两个向量相似度越低时，余弦相似度越接近-1。</p>
<p>下面是用Python实现余弦相似度损失函数的代码示例：</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> torch
<span class="im">import</span> torch.nn <span class="im">as</span> nn

<span class="kw">class</span> CosineSimilarityLoss(nn.Module):
    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):
        <span class="bu">super</span>(CosineSimilarityLoss, <span class="va">self</span>).<span class="fu">__init__</span>()

    <span class="kw">def</span> forward(<span class="va">self</span>, u, v, target):
        cossim <span class="op">=</span> torch.<span class="bu">sum</span>(u <span class="op">*</span> v, dim<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> (torch.norm(u, dim<span class="op">=</span><span class="dv">1</span>) <span class="op">*</span> torch.norm(v, dim<span class="op">=</span><span class="dv">1</span>))
        loss <span class="op">=</span> torch.mean((cossim <span class="op">-</span> target)<span class="op">**</span><span class="dv">2</span>)
        <span class="cf">return</span> loss</code></pre></div>
<p>其中，<code>u</code>和<code>v</code>分别是两个向量，<code>target</code>是它们的真实余弦相似度。在<code>forward</code>方法中，我们首先使用<code>torch.sum</code>计算向量<code>u</code>和<code>v</code>的内积，然后使用<code>torch.norm</code>计算它们的模长，最后将它们相除得到余弦相似度。然后，我们使用均方误差（MSE）计算预测值与真实值之间的差距，并返回损失值。</p>
<h2 id="focal-loss">6. Focal Loss</h2>
<p>Focal Loss是一种用于解决类别不平衡问题的损失函数，它通过对易分类样本的权重进行调整，使得难以分类的样本的权重更大，从而提高了模型对难样本的分类能力。Focal Loss的提出可以有效地缓解类别不平衡问题，提高了模型的泛化性能，在目标检测、图像分类等领域得到了广泛应用。</p>
<p>Focal Loss的原理是通过引入一个缩放因子，对易分类样本的损失进行缩小，从而使得模型更加关注难以分类的样本。具体地，Focal Loss可以通过以下公式来计算损失：</p>
<p><span class="math inline">\(FL(p_t) = -\alpha_t(1-p_t)^\gamma\log(p_t)\)</span></p>
<p>其中，<span class="math inline">\(p_t\)</span>表示模型对当前样本的预测概率，<span class="math inline">\(\alpha_t\)</span>表示对应的样本权重，<span class="math inline">\(\gamma\)</span>表示缩放因子。</p>
<p>当<span class="math inline">\(\gamma=0\)</span>时，Focal Loss退化为标准的交叉熵损失函数；当<span class="math inline">\(\gamma&gt;0\)</span>时，Focal Loss对易分类的样本进行了缩小，从而使得难以分类的样本在损失函数中所占的权重更大。</p>
<p>下面是使用Python实现Focal Loss的代码示例：</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> torch.nn <span class="im">as</span> nn
<span class="im">import</span> torch.nn.functional <span class="im">as</span> F
<span class="im">import</span> torch

<span class="kw">class</span> FocalLoss(nn.Module):
    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, alpha<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span><span class="dv">2</span>, reduction<span class="op">=</span><span class="st">&#39;mean&#39;</span>):
        <span class="bu">super</span>(FocalLoss, <span class="va">self</span>).<span class="fu">__init__</span>()
        <span class="va">self</span>.alpha <span class="op">=</span> alpha
        <span class="va">self</span>.gamma <span class="op">=</span> gamma
        <span class="va">self</span>.reduction <span class="op">=</span> reduction

    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>, target):
        ce_ <span class="op">=</span> F.cross_entropy(<span class="bu">input</span>, target, reduction<span class="op">=</span><span class="st">&#39;none&#39;</span>)
        pt <span class="op">=</span> torch.exp(<span class="op">-</span>ce_)
        f_ <span class="op">=</span> <span class="va">self</span>.alpha <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> pt) <span class="op">**</span> <span class="va">self</span>.gamma <span class="op">*</span> ce_
        <span class="cf">if</span> <span class="va">self</span>.reduction <span class="op">==</span> <span class="st">&#39;mean&#39;</span>:
            <span class="cf">return</span> torch.mean(f_)
        <span class="cf">elif</span> <span class="va">self</span>.reduction <span class="op">==</span> <span class="st">&#39;sum&#39;</span>:
            <span class="cf">return</span> torch.<span class="bu">sum</span>(f_)</code></pre></div>
<p>在上述代码中，我们首先定义了一个FocalLoss类，并实现了<code>__init__</code>和<code>forward</code>方法。其中，<code>alpha</code>和<code>gamma</code>分别表示Focal Loss的两个超参数，<code>reduction</code>表示损失函数的缩减方式。</p>
<p>在<code>forward</code>方法中，我们首先对输入的预测结果进行softmax操作，并计算对数概率（log probability）和概率（probability）。然后，我们根据目标标签的值，计算对应的权重<span class="math inline">\(\alpha\)</span>。接着，我们计算缩放因子<span class="math inline">\(focal\_weight=(1-p_t)^\gamma\)</span>。最后，我们根据上述公式计算Focal Loss，并</p>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  常见网络的代码实现
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
                激活函数
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			el: '#vcomments',
			appId: 'IGK6A3UjpP5uO7JWtA2JoBuS-gzGzoHsz',
			appKey: 'RRf6JtF145uakqoa7Hv1Ahr8',
			placeholder: '请输入评论',
			path: window.location.pathname,
			avatar: 'retro',
			highlight: false,
      recordIP: true,
      enableQQ: true,
			requiredFields: ['nick','mail']
		})
	
	
    // 显示次数
		function showTime(Counter) {
			var query = new AV.Query("Counter");
			if($(".leancloud_visitors").length > 0){
				var url = $(".leancloud_visitors").attr('id').trim();
				// where field
				query.equalTo("words", url);
				// count
				query.count().then(function (number) {
					// There are number instances of MyClass where words equals url.
					$(document.getElementById(url)).text(number?  number : '--');
				}, function (error) {
					// error is an instance of AVError.
				});
			}
		}
		// 追加pv
		function addCount(Counter) {
			var url = $(".leancloud_visitors").length > 0 ? $(".leancloud_visitors").attr('id').trim() : 'wujun234.github.io';
			var Counter = AV.Object.extend("Counter");
			var query = new Counter;
			query.save({
				words: url
			}).then(function (object) {
			})
		}
		$(function () {
			var Counter = AV.Object.extend("Counter");
			addCount(Counter);
			showTime(Counter);
		});
	
</script>
	</div>
	<div id="footer">
	<p>
	©2019-<span id="footerYear"></span> 
	<a href="/">wang yaqi</a>
	|<a href="https://beian.miit.gov.cn" target="_blank">京ICP备2022000211号-1</a>	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>