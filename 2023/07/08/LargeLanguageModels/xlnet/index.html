<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		xlnet | 
	 
	Wangyaqi&#39;s personal site &lt;span style=&#34;color:wheat;font-size:x-small&#34;&gt;图片浏览须科学上网,公式渲染需要刷新&lt;/span&gt;
	</title>
	
	<!-- keywords,description -->
	 
		<meta name="description" content="about study notes" />
	

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	


	<!-- search -->
	<script>
		var searchEngine = "https://www.google.com/search?q=";
		if(typeof searchEngine == "undefined" || searchEngine == null || searchEngine == ""){
			searchEngine = "https://www.google.com/search?q=";
		}
		var homeHost = "wujun234.github.io";
		if(typeof homeHost == "undefined" || homeHost == null || homeHost == ""){
			homeHost = window.location.host;
		}
	</script>


	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">


	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@v1.4.14/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	

	<link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>
	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3efe99c287df5a1d6f0d02d187e403c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<header id="header">
    <a id="title" href="/" class="logo">Wangyaqi's personal site <span style="color:wheat;font-size:x-small">图片浏览须科学上网,公式渲染需要刷新</span></a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
	
		<li class="menu-item">
			<a href="/tags" class="menu-item-link">标签</a>
		</li>
	

	
		<li class="menu-item">
			<a href="/categories" class="menu-item-link">分类</a>
		</li>
	

		<li class="menu-item">
			<a href="https://github.com/wujun234/uid-generator-spring-boot-starter" class="menu-item-link" target="_blank">
				UidGenerator
			</a>
		</li>
		<li class="menu-item">
			<a href="https://github.com/wujun234" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="search" placeholder="按回车全站搜索">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										LargeLanguageModels
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/09/04/LargeLanguageModels/DPO/">
                     
										    DPO
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/LLaMA%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/">
                     
										    LLaMA模型架构
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/LoRA/">
                     
										    LoRA
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/04/LargeLanguageModels/Toolformer/">
                     
										    Toolformer
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/alpaca_LoRA%E5%AE%9E%E7%8E%B0/">
                     
										    alpaca_LoRA实现
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/05/LargeLanguageModels/gpt_training/">
                     
										    gpt_training
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/05/LargeLanguageModels/llama2/">
                     
										    llama2
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/LargeLanguageModels/llama_bloom_chatglm%E5%8C%BA%E5%88%AB/">
                     
										    llama_bloom_chatglm区别
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/llama_visualization/">
                     
										    llama_visualization
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/peft_model/">
                     
										    peft_model
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/LargeLanguageModels/transformer/">
                     
										    transformer
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file active">
									<a href="/2023/07/08/LargeLanguageModels/xlnet/">
                     
										    xlnet
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/05/LargeLanguageModels/%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%89%A9%E5%B1%95%E9%95%BF%E5%BA%A6%E7%9A%84%E5%8F%98%E5%8C%96/">
                     
										    不同的位置编码扩展长度的变化
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/04/LargeLanguageModels/%E7%A8%80%E7%96%8F%E5%8F%98%E6%8D%A2%E5%99%A8/">
                     
										    稀疏变换器
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/05/LargeLanguageModels/%E9%80%9A%E7%94%A8%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96UIE/">
                     
										    通用信息抽取UIE
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/04/LargeLanguageModels/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%8F%98%E7%A7%8D%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF%E5%92%8C%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/">
                     
										    预训练模型变种的设计思路和建模方法
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										机器学习
									</a>
									
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										kaggle_note
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kaggle_note/text_classification/">
                     
										    text_classification
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										nlp
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/nlp/crf/">
                     
										    crf
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										paper
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/12/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/paper/asr%E8%AF%84%E4%BC%B0/">
                     
										    asr评估
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										深度学习
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GBRANK/">
                     
										    GBRANK
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/09/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/HNSW%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E7%9A%84%E5%8E%9F%E7%90%86/">
                     
										    HNSW向量检索的原理
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/MMoE/">
                     
										    MMoE
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/attention/">
                     
										    attention
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/q-learning/">
                     
										    q-learning
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BC%98%E5%8C%96%E5%99%A8/">
                     
										    优化器
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
                     
										    常见网络的代码实现
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0ppo/">
                     
										    强化学习ppo
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/07/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0ppo%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/">
                     
										    强化学习ppo代码阅读
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/">
                     
										    损失函数
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">
                     
										    激活函数
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">
                     
										    知识蒸馏
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										项目集合
									</a>
									
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/llama_%E5%B0%8F%E5%AD%A6%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%A2%98%E6%A8%A1%E5%9E%8B/">
                     
										    llama_小学数学解题模型
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/stableDiffusion/">
                     
										    stableDiffusion
                     
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">
	xlnet
</h1>
<div class="article-meta">
	
		<span>
			阅读量:<span id="/2023/07/08/LargeLanguageModels/xlnet/" class="leancloud_visitors" data-flag-title="xlnet"></span>
		</span>
	
	<span>wang yaqi</span>
	<span>2023-07-08 21:04:50</span>
		<div id="article-categories">
    
		<span>Categories：</span>
            
    

    
		<span>Tags：</span>
            
    
		</div>

</div>

<div id="article-content">
	<h1 id="xlnetgeneralized-autoregressive-pretraining-for-language-understanding">XLNet：Generalized Autoregressive Pretraining for Language Understanding</h1>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/awesome-DeepLearning/blob/master/docs/tutorials/pretrain_model/XLNet.md">转自paddledoc</a></p>
</blockquote>
<h2 id="从ar和ae模型到xlnet模型">1. 从AR和AE模型到XLNet模型</h2>
<p><strong>自回归模型（Autoregressive Model, AR）</strong>，通过估计一串文本序列的生成概率分布进行建模。一般而言，AR模型通过要么从前到后计算文本序列概率，要么从后向前计算文本序列概率，但不论哪种方式的建模，都是单向的。即<strong>在预测一个单词的时候无法同时看到该单词位置两边的信息</strong>。假设给定的文本序列<span class="math inline">\(x=(x_1, x_2, ..., x_n)\)</span>，其从左到右的序列生成概率为：</p>
<p><span class="math display">\[
p(x)=\prod_{t=1}^n p(x_t|x_{&lt;t})
\]</span></p>
<p><strong>自编码模型（Autoencoding Model, AE）</strong>, 通过从破坏的输入文本序列中重建原始数据进行建模。例如BERT通过预测【mask】位置的词重建原始序列。它的<strong>优点</strong>在于在预测单词的时候能够同时捕获该单词位置前后双向的信息；它的<strong>缺点</strong>是预训练过程中采用了mask单词的策略，然而微调阶段并没有，因此导致了预训练阶段和微调阶段的的<strong>GAP</strong>，另外在训练过程中，对不同mask单词的预测是相互独立的。假设序列中被mask的词为<span class="math inline">\(w\in W_m\)</span>，未被mask的词为<span class="math inline">\(w\in W_n\)</span>，则其相应的计算概率为：</p>
<p><span class="math display">\[
p(x) = \prod_{w\in Wm} p(w|W_n)
\]</span></p>
<p>这里对传统的AR和AE模型简单<strong>总结一下</strong>，AR模型是生成式模型，是单向的；AE模型是判别式模型，是双向的。鉴于传统的AR模型和AE模型自身的优点和缺点，<strong>XLNet</strong>期望能够融合两者的优点同时又避免两者的缺点，这就是<strong>XLNet</strong>的设计思路。</p>
<p>整体上<strong>XLNet</strong>是基于AR模型的建模思路设计的，同时避免了只能单向建模的缺点，因此它是一种能看得见双向信息的<strong>广义AR模型</strong>。作为一个AR模型，<strong>XLNet</strong>并没有采用预测mask单词的方式进行建模，因此它不存在上述预训练-微调的GAP，更不存在预测mask的独立性假设。</p>
<h2 id="permutation-language-model">2. Permutation Language Model</h2>
<p>Permuatation Language Mode (下文将简称为PLM) 是XLNet的核心建模思路，在正式介绍之前，我们再来回顾一下AR模型的建模策略，给定一串文本序列<span class="math inline">\(\text{x}=[x_1,x_2,...,x_n]\)</span>，其中每个<span class="math inline">\(x_i\)</span>表示一个token，AR模型的通过最大化下边这个似然函数进行建模：</p>
<p><span class="math display">\[
\begin{align}
\mathop{max}\limits_{\theta} \quad log \; P_{\theta}(\text{x}) &amp;= \sum_{t=1}^{n}log\;p_{\theta}(x_t|x_{&lt;t}) \\&amp;=\sum_{t=1}^{n}log\;\frac{exp(h_{\theta}(x_{1:t-1})^{T}e(x_t))}{\sum_{x^{&#39;}}exp(h_{\theta}(x_{1:t-1})^{T}e(x^{&#39;}))}
\end{align}
\]</span></p>
<p>这里，<span class="math inline">\(\text{x}_{&lt;t}\)</span>表示在<span class="math inline">\(t\)</span>位置前边的token序列，<span class="math inline">\(h_{\theta}(\text{x}_{1:t-1})\)</span>表示数据由模型产生的上下文向量，<span class="math inline">\(e(x_t)\)</span>表示token <span class="math inline">\(x_t\)</span>的embedding。</p>
<p>这种建模方式是单向的，为了在预测某个位置单词的时候，能够让模型看见双向的信息，<strong>XLNet</strong>采用了全排列的思路，允许模型在不同文本序列排列上进行建模，但模型的参数在不同序列上是共享的，相当于是模型能够看见<strong>预测位置单词</strong>左右两侧的信息。</p>
<p>举个例子，假设当前有文本序列<span class="math inline">\(\text{x}=[x_1,x_2,x_3]\)</span>，这串序列中共有3个token，这三个token共计有6种排列组合方式，其相关的索引序列为：</p>
<ul>
<li><span class="math inline">\(\text{z}_1 = (1,2,3)\)</span></li>
<li><span class="math inline">\(\text{z}_2=(1,3,2)\)</span></li>
<li><span class="math inline">\(\text{z}_3=(2,1,3)\)</span></li>
<li><span class="math inline">\(\text{z}_4=(2,3,1)\)</span></li>
<li><span class="math inline">\(\text{z}_5=(3,1,2)\)</span></li>
<li><span class="math inline">\(\text{z}_6=(3,2,1)\)</span></li>
</ul>
<p>采用索引序列<span class="math inline">\(\text{z}_1\)</span>的文本序列为<span class="math inline">\(\text{x}=[x_1, x_2, x_3]\)</span>，采用索引序列<span class="math inline">\(\text{z}_2\)</span>的文本序列为<span class="math inline">\(\text{x}=[x_1, x_3, x_2]\)</span>，如果模型在训练过程中能同时看到这样的两个排列，那么在预测<span class="math inline">\(x_2\)</span>的时候将可以看到<span class="math inline">\(x_1\)</span>，又可以看到<span class="math inline">\(x_3\)</span>，因为训练过程中模型的参数是共享的，因此相当于模型能够看到<span class="math inline">\(x_2\)</span>前后双向的信息。</p>
<p>下面正式归纳一下XLNet的目标函数，假设给定一串序列<span class="math inline">\(\text{x}=[x_1,x_2,x_3,...,x_n]\)</span>，它将有<span class="math inline">\(n!\)</span>个不同的排列组合<span class="math inline">\(\mathbb{Z}=[\text{z}_1,\text{z}_2,...,\text{z}_{n!}]\)</span>，令<span class="math inline">\(\text{z}\in \mathbb{Z}\)</span>表示某一排列方式，<span class="math inline">\(z_{t}\)</span>表示为<span class="math inline">\(\text{z}\)</span>这个排列的第<span class="math inline">\(t\)</span>个位置，<span class="math inline">\(\text{z}_{&lt;t}\)</span>表示<span class="math inline">\(t\)</span>这个位置前边的<span class="math inline">\(t-1\)</span>个位置编号，<span class="math inline">\(\text{x}_{\text{z}}\)</span>表示将序列<span class="math inline">\(\text{x}\)</span>按照索引<span class="math inline">\(\text{z}\)</span>进行排序。<span class="math inline">\(\text{x}_{\text{z}&lt;t}\)</span>表示将原始<span class="math inline">\(\text{x}\)</span>按照<span class="math inline">\(\text{z}\)</span>进行排列后，<span class="math inline">\(\text{x}_{\text{z}}\)</span>前<span class="math inline">\(t-1\)</span>个位置的token。另外假设排列<span class="math inline">\(\text{z}\)</span>出现的概率为<span class="math inline">\(p(\text{z})\)</span>，则<strong>XLNet</strong>的正式目标函数依据极大似然估计为：</p>
<p><span class="math display">\[
\begin{align}
\mathop{max}_{\theta} \quad L &amp;=\mathbb{E}_{\text{z}∼\mathbb{Z}} \left[ \sum_{t=1}^n log\;p_{\theta}(x_{z_t}|\text{x}_{\text{z}_{&lt;t}}) \right] \\
&amp; = \sum_{_{\text{z}∼\mathbb{Z}}} p(\text{z}) \left[  \sum_{t=1}^n log\;p_{\theta}(x_{z_t}|\text{x}_{\text{z}_{&lt;t}})  \right]
\end{align}
\]</span></p>
<p>但是一个长度为<span class="math inline">\(n\)</span>的文本序列，其排列组合数为<span class="math inline">\(n!\)</span>，数量实在庞大，不便训练，所以在实际训练过程中，XLNet是通过采样的方式逼近目标函数的期望，即：</p>
<p><span class="math display">\[
\begin{align}
\mathop{max}_{\theta} \quad L &amp;=\mathbb{E}_{\text{z}∼\mathbb{Z}} \left[ \sum_{t=1}^n log\;p_{\theta}(x_{z_t}|\text{x}_{z&lt;t}) \right] \\
&amp;\approx \frac{1}{m}\sum_{i=1}^{m}\sum_{t=1}^n log\;p_{\theta}(x_{z_{it}}|\text{x}_{\text{z}_i&lt;t})
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(z_{it}\)</span>表示第<span class="math inline">\(i\)</span>个排列的第<span class="math inline">\(t\)</span>个位置，<span class="math inline">\(\text{x}_{\text{z}_i&lt;t}\)</span>表示按照<span class="math inline">\(\text{z}_i\)</span>排列进行重塑后的前<span class="math inline">\(t-1\)</span>个token。每次采样一个排列顺序<span class="math inline">\(\text{z}\)</span>进行重塑原始序列<span class="math inline">\(\text{x}_{\text{z}}\)</span>，然后将<span class="math inline">\(\text{x}_\text{z}\)</span>进行分解组合计算：<span class="math inline">\(\sum_{t=1}^n log\;p_{\theta}(x_{z_t}|\text{x}_{\text{z}&lt;t})\)</span>。这里需要<strong>注意</strong>的是，XLNet只是调整了联合概率<span class="math inline">\(p(\text{x})\)</span>的分解顺序，但是原始token之间的顺序是不变的，即token之间还是使用原始顺序的position embedding，<span class="math inline">\(p(\text{x})\)</span>的分解顺序的计算主要是通过transformer的<strong>mask机制</strong>实现的。这样的设定也保证了预训练阶段和微调阶段之间的顺序是一致的，均是正常的自然语序。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/1649759610/images_for_blog/master/image-20210602163126098.png" alt="image-20210602163126098" />
<p class="caption">image-20210602163126098</p>
</div>
<center>
图1 不同排列计算第3个单词输出的示意图
</center>
<p><strong>图1</strong>中<span class="math inline">\(mem^{(0)}\)</span>和<span class="math inline">\(mem^{(1)}\)</span>代表前一个的<strong>segment</strong>的缓存信息，另外，输入的序列顺序都是固定的自然语序，position embedding还是按照正常的顺序确定的。只是不同的排列中，参与分解计算的内容不同。具体来讲，对第一个分解次序<span class="math inline">\(3\rightarrow2\rightarrow4\rightarrow1\)</span>，因为<span class="math inline">\(x_3\)</span>位于最前边，所以在这个分解次序中看不到其他的token内容，只能看到前一个segment的缓存信息；对第一个分解次序<span class="math inline">\(2\rightarrow4\rightarrow3\rightarrow1\)</span>，<span class="math inline">\(x_3\)</span>前边有<span class="math inline">\(x_2\)</span>和<span class="math inline">\(x_4\)</span>，所以在计算<span class="math inline">\(x_3\)</span>位置输出的时候使用了<span class="math inline">\(x_2\)</span>和<span class="math inline">\(x_4\)</span>。</p>
<p>这个想法就是PLM的建模思路，看到这里，相信你已经很好地理解了。</p>
<h2 id="permutation-language-model如何建模">3. Permutation Language Model如何建模</h2>
<h3 id="使用经典的transformer是否能建模plm">3.1 使用经典的transformer是否能建模PLM</h3>
<p>上边看似找到了一个比较好想法去让AR模型在预测一个单词的时候同时能够看到前后双向的信息，但具体怎么来建模实现呢？使用原始的transformer技术直接实现可能会带来一些问题，具体来说，假设当前有两个排列<span class="math inline">\(\text{z}^{1}\)</span>和<span class="math inline">\(\text{z}^2\)</span>，他们之间具有如下的关系：</p>
<p><span class="math display">\[
\text{z}_{&lt;t}^{1} = \text{z}_{&lt;t}^{2}=\text{z}_{&lt;t} \qquad but \qquad z_t^1=i \neq j= z_t^2
\]</span></p>
<p>这种情况下，使用经典transformer的方式去预测这两个排列<span class="math inline">\(z_t\)</span>位置的输出，将会有：</p>
<p><span class="math display">\[
\underbrace{p_{\theta}(X_{i}=x|\text{x}_{\text{z}_{&lt;t}}) }_{z_t^1=i, \; \text{z}_{&lt;t}^1=\text{z}_{&lt;t}} = \underbrace{p_{\theta}(X_{j}=x|\text{x}_{\text{z}_{&lt;t}}) }_{z_t^2=i, \; \text{z}_{&lt;t}^2=\text{z}_{&lt;t}} = \frac{exp\,(e(x)^T h(\text{x}_{\text{z}_{&lt;t}}))}{\sum_{x^{&#39;}}exp\,(e(x^{&#39;})^T h(\text{x}_{\text{z}_{&lt;t}}))}
\]</span></p>
<p>显然在这种情况下，预测第<span class="math inline">\(i\)</span>个位置的单词和预测第<span class="math inline">\(j\)</span>个位置的单词的概率分布是相同的，这肯定是不对的，因此使用经典的transformer是无法去做Permutation Language Model建模的。</p>
<p>为了解决这个问题，<strong>XLNet</strong>在预测目标<span class="math inline">\(z_t\)</span>位置的token时，向其引入了位置信息<span class="math inline">\(z_t\)</span>，重新定义的预测token概率分布的计算方式为：</p>
<p><span class="math display">\[
p_{\theta}(x_{z_t}|\text{x}_{\text{z}_{&lt;t}})=\frac{exp\,(e(x)^T g_{\theta}(\text{x}_{\text{z}_{&lt;t}},z_{t}))}{\sum_{x^{&#39;}}exp\,(e(x^{&#39;})^T g_{\theta}(\text{x}_{\text{z}_{&lt;t}},z_{t})))}
\]</span></p>
<p>从公式中可以看到，其在预测<span class="math inline">\(z_t\)</span>位置token的时候，引入了位置信息<span class="math inline">\(z_t\)</span>。这样就能解决上述的问题，即经过这个变换后上式将变为：</p>
<p><span class="math display">\[
\underbrace{p_{\theta}(X_{i}=x|\text{x}_{\text{z}_{&lt;t}}) }_{z_t^1=i, \; \text{z}_{&lt;t}^1=\text{z}_{&lt;t}} =\frac{exp\,(e(x)^T g_{\theta}(\text{x}_{\text{z}_{&lt;t}},i))}{\sum_{x^{&#39;}}exp\,(e(x^{&#39;})^T g_{\theta}(\text{x}_{\text{z}_{&lt;t}},i)))} \neq \underbrace{p_{\theta}(X_{j}=x|\text{x}_{\text{z}_{&lt;t}}) }_{z_t^2=i, \; \text{z}_{&lt;t}^2=\text{z}_{&lt;t}}= \frac{exp\,(e(x)^T g_{\theta}(\text{x}_{\text{z}_{&lt;t}},j))}{\sum_{x^{&#39;}}exp\,(e(x^{&#39;})^T g_{\theta}(x_{\text{z}_{&lt;t}},j)))}
\]</span></p>
<h3 id="使用two-stream-self-attention建模plm">3.2 使用Two-Stream Self-Attention建模PLM</h3>
<p>从上边讨论的这些可以看到，当预测<span class="math inline">\(z_t\)</span>位置的token时，最多只能使用位置信息<span class="math inline">\(z_t\)</span>，而不能使用该<span class="math inline">\(z_t\)</span>对应的内容<span class="math inline">\(x_{z_t}\)</span>，否则，就相当于使用<span class="math inline">\(x_{z_t}\)</span>来预测<span class="math inline">\(x_{z_t}\)</span>自己，这没有什么意义；当预测<span class="math inline">\(j&gt;t\)</span>后的token <span class="math inline">\(x_{z_j}\)</span>时，不仅需要位置信息<span class="math inline">\(z_t\)</span>，同时还需要该位置对应的内容<span class="math inline">\(x_{z_t}\)</span>。</p>
<p>然而，经典的transformer中是将两者信息在输入层相加融合之后进行后续计算的，因此<strong>XLNet</strong>提出了一种双流自注意力机制：<strong>content-stream</strong> 和 <strong>query stream</strong>，下面我们将来具体探讨一下它们。</p>
<p><strong>content-stream</strong> 提供了内容方面的表达 content representation <span class="math inline">\(h_{\theta}(\text{x}_{\text{z}_{\leq t}} )\)</span>，简记为<span class="math inline">\(h_{z_t}\)</span>，它等同经典的transformer 的状态向量，这个向量中既包含了位置信息<span class="math inline">\(z_t\)</span>，又包含了内容信息<span class="math inline">\(x_{z_t}\)</span>。</p>
<p><strong>query-stream</strong> 提供了查询方面的表达 query representation <span class="math inline">\(g_{\theta}(\text{x}_{\text{z}_{&lt;t}}, z_t)\)</span>，简记为<span class="math inline">\(g_{z_t}\)</span>，它仅仅包含了<span class="math inline">\(x_{z&lt;t}\)</span>的内容信息和<span class="math inline">\(z_t\)</span>的位置信息，并不包含<span class="math inline">\(x_{z_t}\)</span>的内容。</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/1649759610/images_for_blog/master/image-20210602184721390.png" alt="image-20210602184721390" />
<p class="caption">image-20210602184721390</p>
</div>
<center>
图2 双流机制计算图
</center>
<p><strong>图2</strong>展示了分解顺序为<span class="math inline">\(3 \rightarrow 2 \rightarrow 4 \rightarrow 1\)</span>的<strong>two-stream</strong>计算过程，我们通过这张图来具体聊聊如何去定义<span class="math inline">\(g_{\theta}(\text{x}_{\text{z}_{&lt;t}},z_t)\)</span>。</p>
<p><strong>图2a</strong>展示了<strong>content-stream</strong>的自注意力计算，其中<span class="math inline">\(h_i^{(0)}\)</span>是由token的embedding进行初始化，可以看到它的计算和经典的transormer是一致的，因为在分解顺序中1位于最后，因此它能看见前边所有的token内容，最终计算得出的<span class="math inline">\(h_1^{(1)}\)</span>同时包含了第<span class="math inline">\(1\)</span>个位置的内容信息。</p>
<p><strong>图2b</strong>展示了<strong>query-stream</strong>的自注意力计算，其中<span class="math inline">\(g_i^{(0)}\)</span>由可学习的参数进行初始化，因为它能看见token 3,2,4的内容信息，所以这里用的是内容信息<span class="math inline">\(h_3^{(0)},h_2^{(0)},h_4^{(0)}\)</span>，同时对于第1个位置，只能使用位置信息，而不能使用内容信息，所以这里使用的是<span class="math inline">\(g_1^{(0)}\)</span>，它并不包含第1个位置的内容信息。 这样就能比较好地建模<span class="math inline">\(g_{\theta}(\text{x}_{\text{z}_{&lt;t}},z_t)\)</span>。</p>
<p><strong>图2c</strong>展示了整体的计算流程，最后一层输出的query向量就是我们想要的<span class="math inline">\(g_{\theta}(\text{x}_{\text{z}_{&lt;t}},z_t)\)</span>。右边的两个矩阵分别展示了<strong>content-stream</strong>和<strong>query-stream</strong>的mask矩阵内容，就是使用这样的mask矩阵来计算序列分解式的。 关于这两个流的Self-Attention计算公式如下：</p>
<p><span class="math display">\[
\begin{align}
g_{z_t}^{(m)} &amp; \leftarrow \text{Attention}(Q=g_{z_t}^{(m-1)},\, KV=h_{\text{z}_{&lt;t}}^{(m-1)};\theta), \qquad \text{(query stream: use $z_t$ but cannot see $x_{z_t}$)} \\
h_{z_t}^{(m)} &amp; \leftarrow \text{Attention}(Q=h_{z_t}^{(m-1)},\, KV=h_{\text{z}_{\leq t}}^{(m-1)};\theta), \qquad \text{(content stream: use both $z_t$ and $x_{z_t}$)}
\end{align}
\]</span></p>
<p>以上是XLNet在<strong>预训练</strong>阶段的计算过程，这里需要注意的是，在<strong>微调</strong>阶段，<strong>XLNet</strong>仅仅使用<strong>content respresentation</strong>进行fine-tune下游任务。</p>
<h3 id="引入transformer-xl的想法">3.3 引入Transformer-XL的想法</h3>
<p>由于<strong>XLNet</strong>本身是个AR模型，它可以完美融入Transformer-XL的思想：<strong>相对位置编码</strong>和<strong>segment循环机制</strong>。这两者的原理部分感兴趣的同学可以去阅读Transformer-XL内容，本文重点讨论一下<strong>segment循环机制</strong>向<strong>XLNet</strong>的融入过程。</p>
<p>顾名思义，<strong>segment循环机制</strong>是指长序列切分成<span class="math inline">\(n\)</span>个<strong>segment</strong> (文本片段)，然后将每个<strong>segment</strong>依次传入模型之中，同时传入到模型中，同时传入到模型中还有上一个<strong>segment</strong>的产生的输出，这个操作有点像RNN，接收上一步的输出和当前步骤的输入，然后根据两者计算产生当前步骤的输出，只不过RNN的循环单位是单词，<strong>XLNet</strong>的循环单位是<strong>segment</strong>。</p>
<p>给定一个长序列<span class="math inline">\(\text{s}\)</span>，上一个<strong>segment</strong>为<span class="math inline">\(\tilde{\text{x}}=s_{1:n}\)</span>，其对应的排列用<span class="math inline">\(\tilde{\text{z}}\)</span>表示；当前的<strong>segment</strong>为<span class="math inline">\(\text{x}=s_{n+1:2n}\)</span>，其对应的排列用<span class="math inline">\(\text{z}\)</span>表示。基于排列<span class="math inline">\(\tilde{\text{z}}\)</span>处理第1个<strong>segment</strong>，并将其输出进行缓存，第<span class="math inline">\(m\)</span>层的输出用<span class="math inline">\(\tilde{h^{(m)}}\)</span>表示。则第2个<strong>segment</strong>的计算可以按照如下方式进行：</p>
<p><span class="math display">\[
\begin{align}
g_{z_t}^{(m)} &amp; \leftarrow \text{Attention}(Q=g_{z_t}^{(m-1)},\, KV=[\tilde{h}^{(m-1)},h_{\text{z}_{&lt;t}}^{(m-1)}];\, \theta), \qquad \text{(query stream: use $z_t$ but cannot see $x_{z_t}$)} \\
h_{z_t}^{(m)} &amp; \leftarrow \text{Attention}(Q=h_{z_t}^{(m-1)},\, KV=[\tilde{h}^{(m-1)},h_{\text{z}_{\leq t}}^{(m-1)}];\, \theta), \qquad \text{(content stream: use both $z_t$ and $x_{z_t}$)}
\end{align}
\]</span></p>
<p>即将前一个segment的输出和当前位置<span class="math inline">\(z_t\)</span>能看到的内容进行拼接，然后进行Self-Attention融合计算。</p>
<p>这里需要注意的是，由于序列中的<strong>position embedding</strong> 仅仅依赖于原始序列（输入序列）的位置，而不是排列的顺序，所以一旦前一个<strong>segment</strong>的输出<span class="math inline">\(\tilde{h}^{(m)}\)</span>确定，上述的Attention计算和前一个<strong>segment</strong>的分解顺序无关。这允许我们去缓存或者复用前一个<strong>segment</strong>的输出，而不用去管前一个<strong>segment</strong>的分解顺序。</p>
<h3 id="关于xlnet的一些trick">3.4 关于XLNet的一些Trick</h3>
<h4 id="partial-prediction">3.4.1 Partial Prediction</h4>
<p>最开始的时候有提到，<strong>AR模型</strong>通过估计一串文本序列的生成概率分布进行建模：<span class="math inline">\(\sum_{t=1}^n log\;p_{\theta}(x_{z_t}|\text{x}_{\text{z}&lt;t})\)</span>。PLM虽然解决了AR模型建模过程中的双向问题，但是由于通过这种排列组合的形式训练，导致<strong>XLNet</strong>收敛会比较慢。</p>
<p>因此<strong>XLNet</strong>在训练过程中，只选择预测序列最后面的部分位置的token，这里涉及到一个切分点位置<span class="math inline">\(c\)</span>，它将指示不预测在<span class="math inline">\(c\)</span>前边的位置<span class="math inline">\(\text{z}_{\leq c}\)</span>，只预测<span class="math inline">\(c\)</span>后边的位置<span class="math inline">\({\text{z}_{&gt;c}}\)</span>。<strong>XLNet</strong>中切分点<span class="math inline">\(c\)</span> 的选择由超参数<span class="math inline">\(K\)</span>来确定，<span class="math inline">\(K \approx \frac{n}{n-c}\)</span>，其中<span class="math inline">\(n\)</span>为序列长度。<span class="math inline">\(K\)</span>越大，则需要预测的token数量越少。</p>
<p>这就是Partial Prediction 部分预测，对于切分点<span class="math inline">\(c\)</span>之前的token无需计算query representation，这会大大节省内存，加快模型训练速度。加入切分点后，<strong>XLNet</strong>的目标函数将变为：</p>
<p><span class="math display">\[
\begin{align}
\mathop{max}_{\theta} \quad \mathbb{E}_{\text{z}∼\mathbb{Z}} log \, p_{\theta}(\text{x}_{z_{&gt;c}}|\text{x}_{\leq c}) = \mathop{max}_{\theta}\quad \mathbb{E}_{\text{z}∼\mathbb{Z}} \left[ \sum_{t=c+1}^n log\;p_{\theta}(x_{z_t}|\text{x}_{\text{z}_{&lt;t}}) \right] 
\end{align}
\]</span></p>
<h4 id="multiple-segment-input">3.4.2 Multiple Segment Input</h4>
<p>许多下游任务存在多段输入的情况，比如QA任务中包含query( 简记为A )和answer (简记为B)两个部分，数据的输入形式同BERT一致：<span class="math inline">\(\text{[CLS, A, SEP, B, SEP]}\)</span>。</p>
<p>但是在<strong>segment</strong>循环的时候，每个部分仅仅使用对应上下文的状态缓存。</p>
<h4 id="relative-segment-encoding">3.4.3 Relative Segment Encoding</h4>
<p>Relative Segment Encoding (相对段编码) ， 这里的Segment不是上边将的将序列划分为固定的Segment，而是指输入数据的不同部分，例如<span class="math inline">\(\text{[CLS, A, SEP, B, SEP]}\)</span>，<span class="math inline">\(\text{A}\)</span>和<span class="math inline">\(\text{B}\)</span>分别属于不同的Segment。</p>
<p>BERT直接使用了绝对编码，直接给<span class="math inline">\(\text{A}\)</span>和<span class="math inline">\(\text{B}\)</span>中的token依次设置了0和1，用来指示整个序列中<span class="math inline">\(\text{A}\)</span>和<span class="math inline">\(\text{B}\)</span>是不同的<strong>segment</strong>，即是不同的文本段，例如一个是query，另一个是answer。</p>
<p><strong>XLNet</strong>与BERT不同，它使用了相对编码，给定序列中的两个位置<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>，判断这两个位置对应的token是否在同一个<strong>segment</strong>里面，如果两者在同一个segment里面，<span class="math inline">\(s_{ij}=s_+\)</span>，否则<span class="math inline">\(s_{ij}=s_-\)</span>。 当预测第<span class="math inline">\(i\)</span>个位置token的时候，需要计算用<span class="math inline">\(i\)</span>位置的向量向另一位置<span class="math inline">\(j\)</span>做attention获取分数，其按照如下公式计算：</p>
<p><span class="math display">\[
\alpha_{i,j} = (q_i+b)^T s_{ij}
\]</span></p>
<p>其中<span class="math inline">\(q_i\)</span>为第<span class="math inline">\(i\)</span>个位置的查询向量，<span class="math inline">\(b\)</span>是一个可学习的参数。最终<span class="math inline">\(a_{i,j}\)</span>将被加到正常Self-Attention的注意力分数上。</p>
<p>使用相对段编码有这样的优势：</p>
<ul>
<li>模型的泛化效果会更好;</li>
<li>在微调任务上，它支持超过两个<strong>segment</strong>输入的下游任务（虽然预训练过程中使用了两个<strong>segment</strong>）;</li>
</ul>
<h2 id="相关资料">4. 相关资料</h2>
<ol style="list-style-type: decimal">
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.08237.pdf">XLNet：Generalized Autoregressive Pretraining for Language Understanding</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zihangdai/xlnet">XLNet Github</a></li>
</ol>

</div>


    <div class="post-guide">
        <div class="item left">
            
              <a href="/2023/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0ppo/">
                  <i class="fa fa-angle-left" aria-hidden="true"></i>
                  强化学习ppo
              </a>
            
        </div>
        <div class="item right">
            
              <a href="/2023/05/08/%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/llama_%E5%B0%8F%E5%AD%A6%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%A2%98%E6%A8%A1%E5%9E%8B/">
                llama 小学数学解题模型
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			el: '#vcomments',
			appId: 'IGK6A3UjpP5uO7JWtA2JoBuS-gzGzoHsz',
			appKey: 'RRf6JtF145uakqoa7Hv1Ahr8',
			placeholder: '请输入评论',
			path: window.location.pathname,
			avatar: 'retro',
			highlight: false,
      recordIP: true,
      enableQQ: true,
			requiredFields: ['nick','mail']
		})
	
	
    // 显示次数
		function showTime(Counter) {
			var query = new AV.Query("Counter");
			if($(".leancloud_visitors").length > 0){
				var url = $(".leancloud_visitors").attr('id').trim();
				// where field
				query.equalTo("words", url);
				// count
				query.count().then(function (number) {
					// There are number instances of MyClass where words equals url.
					$(document.getElementById(url)).text(number?  number : '--');
				}, function (error) {
					// error is an instance of AVError.
				});
			}
		}
		// 追加pv
		function addCount(Counter) {
			var url = $(".leancloud_visitors").length > 0 ? $(".leancloud_visitors").attr('id').trim() : 'wujun234.github.io';
			var Counter = AV.Object.extend("Counter");
			var query = new Counter;
			query.save({
				words: url
			}).then(function (object) {
			})
		}
		$(function () {
			var Counter = AV.Object.extend("Counter");
			addCount(Counter);
			showTime(Counter);
		});
	
</script>
	</div>
	<div id="footer">
	<p>
	©2019-<span id="footerYear"></span> 
	<a href="/">wang yaqi</a>
	|<a href="https://beian.miit.gov.cn" target="_blank">京ICP备2022000211号-1</a>	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>